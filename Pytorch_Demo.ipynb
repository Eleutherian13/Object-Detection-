{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hR2r2Ng4UyW0"
      },
      "source": [
        "# PyTorch Basics: Tensors & Gradients\n",
        "\n",
        "<img src=\"data:image/png;base64,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\" width=\"600\"\n",
        "     height=\"300\">\n",
        "\n",
        "\n",
        "\n",
        "  PyTorch is an open-source machine learning framework that is primarily used for developing and training deep learning models. It was developed by Facebook's AI Research Lab and released in 2016. PyTorch provides a flexible and dynamic approach to building neural networks, making it a popular choice among researchers and developers.\n",
        "\n",
        "The framework is built on a dynamic computational graph concept, which means that the graph is built and modified on-the-fly as the program runs. This allows for more intuitive and flexible model development, as you can use standard Python control flow statements and debug the model easily.\n",
        "\n",
        "PyTorch supports automatic differentiation, which enables efficient computation of gradients for training neural networks using backpropagation. It provides a rich set of tools and libraries for tasks such as data loading, model building, optimization, and evaluation.\n",
        "\n",
        "One of the key advantages of PyTorch is its support for GPU acceleration, allowing you to train models on GPUs to significantly speed up computations. It also has a large and active community, which means there are plenty of resources, tutorials, and pre-trained models available.\n",
        "\n",
        "PyTorch is often compared to TensorFlow, another popular deep learning framework. While TensorFlow focuses more on static computation graphs, PyTorch emphasizes dynamic computation graphs. This fundamental difference in design philosophy gives PyTorch an edge when it comes to flexibility and ease of use.\n",
        "\n",
        "Overall, PyTorch is widely used in the research community and is gaining popularity in industry applications as well. It provides a powerful and user-friendly platform for building and training deep learning models.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "RkPUEaaGU1Wq"
      },
      "outputs": [],
      "source": [
        "import torch"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ir2kHmIoXZEh"
      },
      "source": [
        "# Tensors"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jX_ZLQMGXbb0",
        "outputId": "966d075d-f805-4977-985c-ffb034b46c81"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "tensor(4.)"
            ]
          },
          "execution_count": 2,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# Number/Scalar\n",
        "\n",
        "t1 = torch.tensor(4.)\n",
        "t1\n",
        "\n",
        "# 4. is a shorthand notation for 4.0"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7o5cRisxXpvE",
        "outputId": "6c500f3f-8c27-4368-9faf-a2e0331c819b"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "torch.float32"
            ]
          },
          "execution_count": 3,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "t1.dtype"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "m7lAvfClX13E",
        "outputId": "4bad071c-35a4-4fee-d9b6-04954a8e6251"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "tensor([1., 2., 3., 4.])"
            ]
          },
          "execution_count": 4,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "#Vector\n",
        "t2 = torch.tensor([1., 2, 3, 4])\n",
        "t2"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nVs4Nw1yYIoF",
        "outputId": "d07fc2bc-1d55-4a0c-82b2-5fc59b8d85db"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "tensor([[ 5.,  6.],\n",
              "        [ 7.,  8.],\n",
              "        [ 9., 10.]])"
            ]
          },
          "execution_count": 5,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# Matrix\n",
        "t3 = torch.tensor([\n",
        "    [5., 6],\n",
        "    [7,8],\n",
        "    [9,10]\n",
        "])\n",
        "t3"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5tl0NM6YYVDE",
        "outputId": "26ea6edc-c3de-4922-c6d0-ac54db4d3bdc"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "tensor([[[11, 12, 13],\n",
              "         [14, 15, 16]],\n",
              "\n",
              "        [[17, 18, 19],\n",
              "         [20, 21, 22]]])"
            ]
          },
          "execution_count": 6,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# 3 dimensional array\n",
        "t4 = torch.tensor([\n",
        "    [\n",
        "        [11, 12, 13],\n",
        "        [14, 15, 16]\n",
        "    ],\n",
        "    [\n",
        "        [17,18,19],\n",
        "        [20,21,22]\n",
        "    ]\n",
        "])\n",
        "\n",
        "t4"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gb36gN4DYnRV",
        "outputId": "31c4a2f3-a636-40bb-8601-957e306d3cc4"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "tensor(4.)\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "torch.Size([])"
            ]
          },
          "execution_count": 8,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "print(t1)\n",
        "t1.shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lkYyXzOtY0Z4",
        "outputId": "2fb5c6bf-0946-44d7-e563-6b749f04019d"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "tensor([1., 2., 3., 4.])\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "torch.Size([4])"
            ]
          },
          "execution_count": 9,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "print(t2)\n",
        "t2.shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ml--q2-8Y5cR",
        "outputId": "fffc9be5-0c08-4c6b-a1bf-7b66e736b242"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "tensor([[ 5.,  6.],\n",
            "        [ 7.,  8.],\n",
            "        [ 9., 10.]])\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "torch.Size([3, 2])"
            ]
          },
          "execution_count": 10,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "print(t3)\n",
        "t3.shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-F6r8usWZCEU",
        "outputId": "74ba10e1-bd85-4335-a95d-904ab1dd768c"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "tensor([[[11, 12, 13],\n",
            "         [14, 15, 16]],\n",
            "\n",
            "        [[17, 18, 19],\n",
            "         [20, 21, 22]]])\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "torch.Size([2, 2, 3])"
            ]
          },
          "execution_count": 11,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "print(t4)\n",
        "t4.shape"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SNjjJolmZQYw"
      },
      "source": [
        "# Tensor operation and gradients"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "M5ljXxfDZGTr",
        "outputId": "697e621b-37cc-4e7f-af99-57de8318b8a6"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "(tensor(3.), tensor(4., requires_grad=True), tensor(5., requires_grad=True))"
            ]
          },
          "execution_count": 13,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# Create tensors\n",
        "\n",
        "x = torch.tensor(3.)\n",
        "w = torch.tensor(4. , requires_grad = True)\n",
        "b = torch.tensor(5., requires_grad = True)\n",
        "\n",
        "x, w, b"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OAjtPvmqZnbr",
        "outputId": "7b9bfbbe-a5b7-461a-a8ab-4d16999c8d4b"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "tensor(17., grad_fn=<AddBackward0>)"
            ]
          },
          "execution_count": 14,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# Arithemitc operations\n",
        "\n",
        "y = w*x + b\n",
        "y"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "id": "cd7zwh1WZy9x"
      },
      "outputs": [],
      "source": [
        "# Compute derivates\n",
        "y.backward()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8dEH2Wk1aqul",
        "outputId": "e5203c25-0343-4430-e88e-e0e2892192cc"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "dy/dx:  None\n",
            "dy/dw:  tensor(3.)\n",
            "dy/db:  tensor(1.)\n"
          ]
        }
      ],
      "source": [
        "# Display gradients\n",
        "prin print(Y.grad)t(\"dy/dx: \" ,x.grad)\n",
        "print(\"dy/dw: \", w.grad)\n",
        "print(\"dy/db: \", b.grad)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hKv5BH5sa7lR"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "G_smRE9hba7h"
      },
      "source": [
        "# Tensor functions"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lqPiKgCfbePU",
        "outputId": "3cd80525-561f-4537-81ff-757edaccdc32"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "tensor([[42, 42],\n",
              "        [42, 42],\n",
              "        [42, 42]])"
            ]
          },
          "execution_count": 22,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# Create a tensor with a fixed value for every elemnt\n",
        "\n",
        "t6 = torch.full((3,2), 42)\n",
        "t6"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SKyuNvnsbqdC",
        "outputId": "d3634963-1251-49bc-b5e5-841f3ea9ed05"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "tensor([[ 5.,  6.],\n",
              "        [ 7.,  8.],\n",
              "        [ 9., 10.],\n",
              "        [42., 42.],\n",
              "        [42., 42.],\n",
              "        [42., 42.]])"
            ]
          },
          "execution_count": 23,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# Concatinate two tensors with compatible shapes\n",
        "t7 = torch.cat((t3, t6))\n",
        "t7"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 24,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-uMWQD4ab9TS",
        "outputId": "d191960d-560b-4204-cd3c-b48abadd0868"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "tensor([[-0.9589, -0.2794],\n",
              "        [ 0.6570,  0.9894],\n",
              "        [ 0.4121, -0.5440],\n",
              "        [-0.9165, -0.9165],\n",
              "        [-0.9165, -0.9165],\n",
              "        [-0.9165, -0.9165]])"
            ]
          },
          "execution_count": 24,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# change the sin of each element\n",
        "t8 = torch.sin(t7)\n",
        "t8"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 25,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "n7Kvd7nfcPOQ",
        "outputId": "e749e05f-6663-4377-90cf-25ef94410d84"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "tensor([[[-0.9589, -0.2794],\n",
              "         [ 0.6570,  0.9894]],\n",
              "\n",
              "        [[ 0.4121, -0.5440],\n",
              "         [-0.9165, -0.9165]],\n",
              "\n",
              "        [[-0.9165, -0.9165],\n",
              "         [-0.9165, -0.9165]]])"
            ]
          },
          "execution_count": 25,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# change the shape of a tensor\n",
        "\n",
        "t9 = t8.reshape(3,2,2)\n",
        "t9"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-rzy3EUvckvZ"
      },
      "source": [
        "# Interoperability with Numpy"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 26,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Nrn99D_Ocgtk",
        "outputId": "a1037aff-dbee-4dda-ba36-02fa73eb292e"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "array([[1., 2.],\n",
              "       [3., 4.]])"
            ]
          },
          "execution_count": 26,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "import numpy as np\n",
        "\n",
        "x = np.array([[1,2], [3, 4.]])\n",
        "x"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 27,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3W9w-m_dc0yf",
        "outputId": "f423e220-51f2-459c-e7d2-752ff659c6bd"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "tensor([[1., 2.],\n",
              "        [3., 4.]], dtype=torch.float64)"
            ]
          },
          "execution_count": 27,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "y = torch.from_numpy(x)\n",
        "y"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 28,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UGzPY5Foc5oF",
        "outputId": "a0490ea4-9a9e-4c99-ec90-763a7923591c"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "(dtype('float64'), torch.float64)"
            ]
          },
          "execution_count": 28,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "x.dtype, y.dtype"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 30,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lZkn3mFec-Dv",
        "outputId": "1a0e03ff-19ba-4f8d-f3ec-41e0179fa34d"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "array([[1., 2.],\n",
              "       [3., 4.]])"
            ]
          },
          "execution_count": 30,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "z = y.numpy()\n",
        "z"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "UZSm1V5bdFe7"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Lldq7pw1fKP_"
      },
      "source": [
        "# Linear Regression from Scratch using pytorch"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 31,
      "metadata": {
        "id": "SxVmWrmOfN-a"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import torch"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 32,
      "metadata": {
        "id": "NeqmVPx_qMF_"
      },
      "outputs": [],
      "source": [
        "# Making training data\n",
        "# Input ---> (temp, rainfall, humidity) ---> yield of apple and oranges crops\n",
        "\n",
        "inputs = np.array([\n",
        "    [73, 67,43],\n",
        "    [91, 88, 64],\n",
        "    [87, 134, 58],\n",
        "    [102, 43, 37],\n",
        "    [69, 96, 70],\n",
        "], dtype = 'float32')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 33,
      "metadata": {
        "id": "IloGhLOTtUps"
      },
      "outputs": [],
      "source": [
        "# Target (apples, oranges)\n",
        "\n",
        "target = np.array([\n",
        "    [56, 70],\n",
        "    [81, 101],\n",
        "    [119, 113],\n",
        "    [22, 37],\n",
        "    [103, 119]\n",
        "], dtype = 'float32')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 34,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HJnovyzuts3_",
        "outputId": "cff06175-621f-4204-bf61-02f96c06e865"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "tensor([[ 73.,  67.,  43.],\n",
            "        [ 91.,  88.,  64.],\n",
            "        [ 87., 134.,  58.],\n",
            "        [102.,  43.,  37.],\n",
            "        [ 69.,  96.,  70.]])\n",
            "tensor([[ 56.,  70.],\n",
            "        [ 81., 101.],\n",
            "        [119., 113.],\n",
            "        [ 22.,  37.],\n",
            "        [103., 119.]])\n"
          ]
        }
      ],
      "source": [
        "# Convert inputs and target to tensors\n",
        "\n",
        "inputs = torch.from_numpy(inputs)\n",
        "target = torch.from_numpy(target)\n",
        "\n",
        "print(inputs)\n",
        "print(target)\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 64,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "J4CnNduSuL-r",
        "outputId": "14984f9d-6c9e-4ca7-f8ba-fdace615778b"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "tensor([[-0.4381, -0.3557, -2.1575],\n",
            "        [ 0.3437,  0.7292,  0.9521]], requires_grad=True)\n",
            "tensor([-1.3664, -0.4123], requires_grad=True)\n"
          ]
        }
      ],
      "source": [
        "# Weights and biases\n",
        "w = torch.randn(2, 3, requires_grad = True)\n",
        "b = torch.randn(2, requires_grad = True)\n",
        "\n",
        "print(w)\n",
        "print(b)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 65,
      "metadata": {
        "id": "SeH79rcOufGA"
      },
      "outputs": [],
      "source": [
        "# define the model\n",
        "\n",
        "# Z = X * W + B\n",
        "def model(x):\n",
        "  return x @ w.t() + b\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 66,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2rP-JdPguuuA",
        "outputId": "31cba636-938e-4c35-f3fe-98d98638c68f"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "tensor([[-149.9502,  114.4780],\n",
            "        [-210.6126,  155.9727],\n",
            "        [-212.2760,  182.4292],\n",
            "        [-141.1742,  101.2327],\n",
            "        [-216.7646,  159.9566]], grad_fn=<AddBackward0>)\n"
          ]
        }
      ],
      "source": [
        "# prediction\n",
        "preds = model(inputs)\n",
        "print(preds)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 67,
      "metadata": {
        "id": "soRsVNN2uvii"
      },
      "outputs": [],
      "source": [
        "# loss funtion we will use is MSE -> Mean squared error\n",
        "def MSE(y, y_hat):\n",
        "  diff = y - y_hat\n",
        "\n",
        "  return torch.sum(diff*diff)/diff.numel()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 68,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KX2wH_1KwFjL",
        "outputId": "0a2f562a-4093-4802-c6d2-b347a9ec977d"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "tensor(38169.6367, grad_fn=<DivBackward0>)\n"
          ]
        }
      ],
      "source": [
        "# error\n",
        "loss = MSE(target, preds)\n",
        "print(loss)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 69,
      "metadata": {
        "id": "RlNew9w5waGB"
      },
      "outputs": [],
      "source": [
        "#Compute gradients\n",
        "loss.backward()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 70,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "juu4vIiaw6B5",
        "outputId": "b1f29e3f-30ad-4037-8af8-7e94b5d62652"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "tensor([[-0.4381, -0.3557, -2.1575],\n",
            "        [ 0.3437,  0.7292,  0.9521]], requires_grad=True)\n",
            "tensor([[-21819.9277, -24313.0898, -15030.8076],\n",
            "        [  4733.4976,   4762.9946,   2940.2544]])\n"
          ]
        }
      ],
      "source": [
        "print(w)\n",
        "print(w.grad)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 71,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "AEgUY2xnw8p8",
        "outputId": "f3421df1-294e-4fe6-fc50-737489f05999"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "tensor([-1.3664, -0.4123], requires_grad=True)\n",
            "tensor([-262.3555,   54.8138])\n"
          ]
        }
      ],
      "source": [
        "print(b)\n",
        "print(b.grad)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 72,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Ugiyw7qYxAyM",
        "outputId": "e309743e-d5dd-419f-d20b-9c7d317f14b3"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "tensor([[0., 0., 0.],\n",
            "        [0., 0., 0.]])\n",
            "tensor([0., 0.])\n"
          ]
        }
      ],
      "source": [
        "#reset grad\n",
        "w.grad.zero_()\n",
        "b.grad.zero_()\n",
        "\n",
        "print(w.grad)\n",
        "print(b.grad)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 73,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FMOTwzPzxEHa",
        "outputId": "029d23ef-a358-49de-9e2a-114fc9d81e51"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "tensor([[-149.9502,  114.4780],\n",
            "        [-210.6126,  155.9727],\n",
            "        [-212.2760,  182.4292],\n",
            "        [-141.1742,  101.2327],\n",
            "        [-216.7646,  159.9566]], grad_fn=<AddBackward0>)\n",
            "tensor(38169.6367, grad_fn=<DivBackward0>)\n"
          ]
        }
      ],
      "source": [
        "# Adjust params\n",
        "\n",
        "preds = model(inputs)\n",
        "\n",
        "print(preds)\n",
        "\n",
        "\n",
        "loss = MSE(target, preds)\n",
        "print(loss)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 74,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Zpwhb4ylxTcD",
        "outputId": "e4d2bae8-2c8b-49cb-fff3-844212fb9f25"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "tensor([[-21819.9277, -24313.0898, -15030.8076],\n",
            "        [  4733.4976,   4762.9946,   2940.2544]])\n",
            "tensor([-262.3555,   54.8138])\n"
          ]
        }
      ],
      "source": [
        "loss.backward()\n",
        "\n",
        "print(w.grad)\n",
        "print(b.grad)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 75,
      "metadata": {
        "id": "mWkU36MmxWj9"
      },
      "outputs": [],
      "source": [
        "# adjust weight & reset grad\n",
        "\n",
        "learning_rate = 1e-5\n",
        "\n",
        "with torch.no_grad():\n",
        "  w -= w.grad * 1e-5\n",
        "  b -= b.grad * 1e-5\n",
        "\n",
        "  w.grad.zero_()\n",
        "  b.grad.zero_()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 76,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SQ8wEMmjyU_y",
        "outputId": "fa85c3b7-c3a5-4e0e-9f0b-68ab3786f3e9"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "tensor([[-0.2199, -0.1125, -2.0072],\n",
            "        [ 0.2964,  0.6816,  0.9227]], requires_grad=True)\n",
            "tensor([-1.3637, -0.4128], requires_grad=True)\n"
          ]
        }
      ],
      "source": [
        "print(w)\n",
        "print(b)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 77,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "C8Nt5nc2zKM8",
        "outputId": "4745e7bf-e249-4f5f-b327-d48b2d2ca661"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "tensor(25905.7031, grad_fn=<DivBackward0>)\n"
          ]
        }
      ],
      "source": [
        "# Calculate again\n",
        "\n",
        "preds = model(inputs)\n",
        "loss = MSE(target, preds)\n",
        "print(loss)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 78,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "eVuwJdxMzYYt",
        "outputId": "b937f060-cf06-44ec-fcb5-ff3e0708561e"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epochs(0/100) & Loss 25905.703125\n",
            "Epochs(1/100) & Loss 17639.734375\n",
            "Epochs(2/100) & Loss 12067.9482421875\n",
            "Epochs(3/100) & Loss 8311.7548828125\n",
            "Epochs(4/100) & Loss 5779.07763671875\n",
            "Epochs(5/100) & Loss 4070.928466796875\n",
            "Epochs(6/100) & Loss 2918.4326171875\n",
            "Epochs(7/100) & Loss 2140.399169921875\n",
            "Epochs(8/100) & Loss 1614.726806640625\n",
            "Epochs(9/100) & Loss 1259.1329345703125\n",
            "Epochs(10/100) & Loss 1018.1676025390625\n",
            "Epochs(11/100) & Loss 854.46435546875\n",
            "Epochs(12/100) & Loss 742.8419799804688\n",
            "Epochs(13/100) & Loss 666.3314208984375\n",
            "Epochs(14/100) & Loss 613.4966430664062\n",
            "Epochs(15/100) & Loss 576.6312866210938\n",
            "Epochs(16/100) & Loss 550.5420532226562\n",
            "Epochs(17/100) & Loss 531.7286376953125\n",
            "Epochs(18/100) & Loss 517.8328857421875\n",
            "Epochs(19/100) & Loss 507.26416015625\n",
            "Epochs(20/100) & Loss 498.95196533203125\n",
            "Epochs(21/100) & Loss 492.17352294921875\n",
            "Epochs(22/100) & Loss 486.4419860839844\n",
            "Epochs(23/100) & Loss 481.42938232421875\n",
            "Epochs(24/100) & Loss 476.91400146484375\n",
            "Epochs(25/100) & Loss 472.74700927734375\n",
            "Epochs(26/100) & Loss 468.82720947265625\n",
            "Epochs(27/100) & Loss 465.08648681640625\n",
            "Epochs(28/100) & Loss 461.47900390625\n",
            "Epochs(29/100) & Loss 457.97344970703125\n",
            "Epochs(30/100) & Loss 454.5487365722656\n",
            "Epochs(31/100) & Loss 451.19036865234375\n",
            "Epochs(32/100) & Loss 447.8887634277344\n",
            "Epochs(33/100) & Loss 444.6368713378906\n",
            "Epochs(34/100) & Loss 441.42974853515625\n",
            "Epochs(35/100) & Loss 438.26458740234375\n",
            "Epochs(36/100) & Loss 435.13861083984375\n",
            "Epochs(37/100) & Loss 432.05035400390625\n",
            "Epochs(38/100) & Loss 428.99835205078125\n",
            "Epochs(39/100) & Loss 425.98150634765625\n",
            "Epochs(40/100) & Loss 422.9991760253906\n",
            "Epochs(41/100) & Loss 420.05084228515625\n",
            "Epochs(42/100) & Loss 417.1353454589844\n",
            "Epochs(43/100) & Loss 414.2527770996094\n",
            "Epochs(44/100) & Loss 411.4019470214844\n",
            "Epochs(45/100) & Loss 408.5831298828125\n",
            "Epochs(46/100) & Loss 405.79547119140625\n",
            "Epochs(47/100) & Loss 403.038818359375\n",
            "Epochs(48/100) & Loss 400.31256103515625\n",
            "Epochs(49/100) & Loss 397.6164855957031\n",
            "Epochs(50/100) & Loss 394.9498291015625\n",
            "Epochs(51/100) & Loss 392.3126525878906\n",
            "Epochs(52/100) & Loss 389.7045593261719\n",
            "Epochs(53/100) & Loss 387.12481689453125\n",
            "Epochs(54/100) & Loss 384.57366943359375\n",
            "Epochs(55/100) & Loss 382.0501708984375\n",
            "Epochs(56/100) & Loss 379.5541687011719\n",
            "Epochs(57/100) & Loss 377.0852966308594\n",
            "Epochs(58/100) & Loss 374.64361572265625\n",
            "Epochs(59/100) & Loss 372.2283020019531\n",
            "Epochs(60/100) & Loss 369.8392639160156\n",
            "Epochs(61/100) & Loss 367.47613525390625\n",
            "Epochs(62/100) & Loss 365.1385803222656\n",
            "Epochs(63/100) & Loss 362.8263244628906\n",
            "Epochs(64/100) & Loss 360.5389099121094\n",
            "Epochs(65/100) & Loss 358.2761535644531\n",
            "Epochs(66/100) & Loss 356.0377197265625\n",
            "Epochs(67/100) & Loss 353.8234558105469\n",
            "Epochs(68/100) & Loss 351.6326599121094\n",
            "Epochs(69/100) & Loss 349.4656677246094\n",
            "Epochs(70/100) & Loss 347.3216247558594\n",
            "Epochs(71/100) & Loss 345.20050048828125\n",
            "Epochs(72/100) & Loss 343.10205078125\n",
            "Epochs(73/100) & Loss 341.02569580078125\n",
            "Epochs(74/100) & Loss 338.97161865234375\n",
            "Epochs(75/100) & Loss 336.9393615722656\n",
            "Epochs(76/100) & Loss 334.92828369140625\n",
            "Epochs(77/100) & Loss 332.9388122558594\n",
            "Epochs(78/100) & Loss 330.9703063964844\n",
            "Epochs(79/100) & Loss 329.02215576171875\n",
            "Epochs(80/100) & Loss 327.0948181152344\n",
            "Epochs(81/100) & Loss 325.1875\n",
            "Epochs(82/100) & Loss 323.3002624511719\n",
            "Epochs(83/100) & Loss 321.4328308105469\n",
            "Epochs(84/100) & Loss 319.5846862792969\n",
            "Epochs(85/100) & Loss 317.75592041015625\n",
            "Epochs(86/100) & Loss 315.94635009765625\n",
            "Epochs(87/100) & Loss 314.15533447265625\n",
            "Epochs(88/100) & Loss 312.38287353515625\n",
            "Epochs(89/100) & Loss 310.6287841796875\n",
            "Epochs(90/100) & Loss 308.8929138183594\n",
            "Epochs(91/100) & Loss 307.1746826171875\n",
            "Epochs(92/100) & Loss 305.4742126464844\n",
            "Epochs(93/100) & Loss 303.7911682128906\n",
            "Epochs(94/100) & Loss 302.1253662109375\n",
            "Epochs(95/100) & Loss 300.47662353515625\n",
            "Epochs(96/100) & Loss 298.84466552734375\n",
            "Epochs(97/100) & Loss 297.22930908203125\n",
            "Epochs(98/100) & Loss 295.6304931640625\n",
            "Epochs(99/100) & Loss 294.0478820800781\n",
            "Epochs(100/100) & Loss 292.4811706542969\n",
            "Epochs(101/100) & Loss 290.93017578125\n",
            "Epochs(102/100) & Loss 289.3951416015625\n",
            "Epochs(103/100) & Loss 287.875244140625\n",
            "Epochs(104/100) & Loss 286.37054443359375\n",
            "Epochs(105/100) & Loss 284.88104248046875\n",
            "Epochs(106/100) & Loss 283.4064636230469\n",
            "Epochs(107/100) & Loss 281.9466552734375\n",
            "Epochs(108/100) & Loss 280.5010986328125\n",
            "Epochs(109/100) & Loss 279.0699462890625\n",
            "Epochs(110/100) & Loss 277.653076171875\n",
            "Epochs(111/100) & Loss 276.250244140625\n",
            "Epochs(112/100) & Loss 274.8611755371094\n",
            "Epochs(113/100) & Loss 273.48590087890625\n",
            "Epochs(114/100) & Loss 272.1241149902344\n",
            "Epochs(115/100) & Loss 270.7754821777344\n",
            "Epochs(116/100) & Loss 269.4400939941406\n",
            "Epochs(117/100) & Loss 268.11773681640625\n",
            "Epochs(118/100) & Loss 266.8082275390625\n",
            "Epochs(119/100) & Loss 265.5113525390625\n",
            "Epochs(120/100) & Loss 264.2270812988281\n",
            "Epochs(121/100) & Loss 262.95526123046875\n",
            "Epochs(122/100) & Loss 261.6956787109375\n",
            "Epochs(123/100) & Loss 260.4481506347656\n",
            "Epochs(124/100) & Loss 259.2125244140625\n",
            "Epochs(125/100) & Loss 257.98876953125\n",
            "Epochs(126/100) & Loss 256.7765197753906\n",
            "Epochs(127/100) & Loss 255.57608032226562\n",
            "Epochs(128/100) & Loss 254.3868408203125\n",
            "Epochs(129/100) & Loss 253.20889282226562\n",
            "Epochs(130/100) & Loss 252.04208374023438\n",
            "Epochs(131/100) & Loss 250.8863067626953\n",
            "Epochs(132/100) & Loss 249.74124145507812\n",
            "Epochs(133/100) & Loss 248.60708618164062\n",
            "Epochs(134/100) & Loss 247.48336791992188\n",
            "Epochs(135/100) & Loss 246.3702850341797\n",
            "Epochs(136/100) & Loss 245.26736450195312\n",
            "Epochs(137/100) & Loss 244.17471313476562\n",
            "Epochs(138/100) & Loss 243.09213256835938\n",
            "Epochs(139/100) & Loss 242.0197296142578\n",
            "Epochs(140/100) & Loss 240.95693969726562\n",
            "Epochs(141/100) & Loss 239.904052734375\n",
            "Epochs(142/100) & Loss 238.8607177734375\n",
            "Epochs(143/100) & Loss 237.82687377929688\n",
            "Epochs(144/100) & Loss 236.802490234375\n",
            "Epochs(145/100) & Loss 235.7872772216797\n",
            "Epochs(146/100) & Loss 234.78125\n",
            "Epochs(147/100) & Loss 233.7843780517578\n",
            "Epochs(148/100) & Loss 232.7964630126953\n",
            "Epochs(149/100) & Loss 231.8173370361328\n",
            "Epochs(150/100) & Loss 230.8470001220703\n",
            "Epochs(151/100) & Loss 229.88522338867188\n",
            "Epochs(152/100) & Loss 228.9320526123047\n",
            "Epochs(153/100) & Loss 227.9873504638672\n",
            "Epochs(154/100) & Loss 227.05093383789062\n",
            "Epochs(155/100) & Loss 226.1227264404297\n",
            "Epochs(156/100) & Loss 225.2026824951172\n",
            "Epochs(157/100) & Loss 224.2906036376953\n",
            "Epochs(158/100) & Loss 223.38671875\n",
            "Epochs(159/100) & Loss 222.49057006835938\n",
            "Epochs(160/100) & Loss 221.60220336914062\n",
            "Epochs(161/100) & Loss 220.7213897705078\n",
            "Epochs(162/100) & Loss 219.84823608398438\n",
            "Epochs(163/100) & Loss 218.98257446289062\n",
            "Epochs(164/100) & Loss 218.12429809570312\n",
            "Epochs(165/100) & Loss 217.2733154296875\n",
            "Epochs(166/100) & Loss 216.4296112060547\n",
            "Epochs(167/100) & Loss 215.5930633544922\n",
            "Epochs(168/100) & Loss 214.7635040283203\n",
            "Epochs(169/100) & Loss 213.9410858154297\n",
            "Epochs(170/100) & Loss 213.12539672851562\n",
            "Epochs(171/100) & Loss 212.31668090820312\n",
            "Epochs(172/100) & Loss 211.51449584960938\n",
            "Epochs(173/100) & Loss 210.71926879882812\n",
            "Epochs(174/100) & Loss 209.9302978515625\n",
            "Epochs(175/100) & Loss 209.1480255126953\n",
            "Epochs(176/100) & Loss 208.3720703125\n",
            "Epochs(177/100) & Loss 207.6026153564453\n",
            "Epochs(178/100) & Loss 206.83938598632812\n",
            "Epochs(179/100) & Loss 206.08242797851562\n",
            "Epochs(180/100) & Loss 205.3314971923828\n",
            "Epochs(181/100) & Loss 204.58676147460938\n",
            "Epochs(182/100) & Loss 203.84786987304688\n",
            "Epochs(183/100) & Loss 203.1149139404297\n",
            "Epochs(184/100) & Loss 202.38790893554688\n",
            "Epochs(185/100) & Loss 201.66665649414062\n",
            "Epochs(186/100) & Loss 200.95120239257812\n",
            "Epochs(187/100) & Loss 200.24130249023438\n",
            "Epochs(188/100) & Loss 199.5369415283203\n",
            "Epochs(189/100) & Loss 198.83828735351562\n",
            "Epochs(190/100) & Loss 198.1449432373047\n",
            "Epochs(191/100) & Loss 197.4571075439453\n",
            "Epochs(192/100) & Loss 196.77464294433594\n",
            "Epochs(193/100) & Loss 196.09722900390625\n",
            "Epochs(194/100) & Loss 195.42526245117188\n",
            "Epochs(195/100) & Loss 194.75851440429688\n",
            "Epochs(196/100) & Loss 194.09657287597656\n",
            "Epochs(197/100) & Loss 193.43992614746094\n",
            "Epochs(198/100) & Loss 192.7881622314453\n",
            "Epochs(199/100) & Loss 192.14138793945312\n",
            "Epochs(200/100) & Loss 191.49942016601562\n",
            "Epochs(201/100) & Loss 190.86228942871094\n",
            "Epochs(202/100) & Loss 190.22988891601562\n",
            "Epochs(203/100) & Loss 189.60220336914062\n",
            "Epochs(204/100) & Loss 188.97911071777344\n",
            "Epochs(205/100) & Loss 188.36068725585938\n",
            "Epochs(206/100) & Loss 187.74688720703125\n",
            "Epochs(207/100) & Loss 187.1374053955078\n",
            "Epochs(208/100) & Loss 186.53256225585938\n",
            "Epochs(209/100) & Loss 185.93209838867188\n",
            "Epochs(210/100) & Loss 185.33580017089844\n",
            "Epochs(211/100) & Loss 184.74398803710938\n",
            "Epochs(212/100) & Loss 184.1563262939453\n",
            "Epochs(213/100) & Loss 183.57289123535156\n",
            "Epochs(214/100) & Loss 182.99374389648438\n",
            "Epochs(215/100) & Loss 182.41854858398438\n",
            "Epochs(216/100) & Loss 181.847412109375\n",
            "Epochs(217/100) & Loss 181.2804412841797\n",
            "Epochs(218/100) & Loss 180.71725463867188\n",
            "Epochs(219/100) & Loss 180.15806579589844\n",
            "Epochs(220/100) & Loss 179.60293579101562\n",
            "Epochs(221/100) & Loss 179.05145263671875\n",
            "Epochs(222/100) & Loss 178.5037078857422\n",
            "Epochs(223/100) & Loss 177.9598388671875\n",
            "Epochs(224/100) & Loss 177.4196319580078\n",
            "Epochs(225/100) & Loss 176.88319396972656\n",
            "Epochs(226/100) & Loss 176.3503875732422\n",
            "Epochs(227/100) & Loss 175.821044921875\n",
            "Epochs(228/100) & Loss 175.29541015625\n",
            "Epochs(229/100) & Loss 174.77313232421875\n",
            "Epochs(230/100) & Loss 174.25430297851562\n",
            "Epochs(231/100) & Loss 173.73898315429688\n",
            "Epochs(232/100) & Loss 173.22727966308594\n",
            "Epochs(233/100) & Loss 172.71871948242188\n",
            "Epochs(234/100) & Loss 172.21359252929688\n",
            "Epochs(235/100) & Loss 171.71173095703125\n",
            "Epochs(236/100) & Loss 171.21298217773438\n",
            "Epochs(237/100) & Loss 170.71755981445312\n",
            "Epochs(238/100) & Loss 170.2254180908203\n",
            "Epochs(239/100) & Loss 169.73626708984375\n",
            "Epochs(240/100) & Loss 169.25033569335938\n",
            "Epochs(241/100) & Loss 168.76744079589844\n",
            "Epochs(242/100) & Loss 168.28762817382812\n",
            "Epochs(243/100) & Loss 167.81088256835938\n",
            "Epochs(244/100) & Loss 167.33700561523438\n",
            "Epochs(245/100) & Loss 166.8660888671875\n",
            "Epochs(246/100) & Loss 166.3981475830078\n",
            "Epochs(247/100) & Loss 165.93309020996094\n",
            "Epochs(248/100) & Loss 165.47085571289062\n",
            "Epochs(249/100) & Loss 165.01162719726562\n",
            "Epochs(250/100) & Loss 164.55496215820312\n",
            "Epochs(251/100) & Loss 164.10118103027344\n",
            "Epochs(252/100) & Loss 163.65011596679688\n",
            "Epochs(253/100) & Loss 163.2017364501953\n",
            "Epochs(254/100) & Loss 162.7559356689453\n",
            "Epochs(255/100) & Loss 162.31297302246094\n",
            "Epochs(256/100) & Loss 161.87255859375\n",
            "Epochs(257/100) & Loss 161.43467712402344\n",
            "Epochs(258/100) & Loss 160.99966430664062\n",
            "Epochs(259/100) & Loss 160.56683349609375\n",
            "Epochs(260/100) & Loss 160.1366729736328\n",
            "Epochs(261/100) & Loss 159.70895385742188\n",
            "Epochs(262/100) & Loss 159.28369140625\n",
            "Epochs(263/100) & Loss 158.8609619140625\n",
            "Epochs(264/100) & Loss 158.44052124023438\n",
            "Epochs(265/100) & Loss 158.0226287841797\n",
            "Epochs(266/100) & Loss 157.6070098876953\n",
            "Epochs(267/100) & Loss 157.1937255859375\n",
            "Epochs(268/100) & Loss 156.78271484375\n",
            "Epochs(269/100) & Loss 156.37417602539062\n",
            "Epochs(270/100) & Loss 155.9676971435547\n",
            "Epochs(271/100) & Loss 155.56353759765625\n",
            "Epochs(272/100) & Loss 155.16159057617188\n",
            "Epochs(273/100) & Loss 154.76185607910156\n",
            "Epochs(274/100) & Loss 154.36428833007812\n",
            "Epochs(275/100) & Loss 153.9688720703125\n",
            "Epochs(276/100) & Loss 153.5756072998047\n",
            "Epochs(277/100) & Loss 153.18453979492188\n",
            "Epochs(278/100) & Loss 152.79544067382812\n",
            "Epochs(279/100) & Loss 152.40846252441406\n",
            "Epochs(280/100) & Loss 152.0234375\n",
            "Epochs(281/100) & Loss 151.64053344726562\n",
            "Epochs(282/100) & Loss 151.25955200195312\n",
            "Epochs(283/100) & Loss 150.88067626953125\n",
            "Epochs(284/100) & Loss 150.5037841796875\n",
            "Epochs(285/100) & Loss 150.12872314453125\n",
            "Epochs(286/100) & Loss 149.75576782226562\n",
            "Epochs(287/100) & Loss 149.38438415527344\n",
            "Epochs(288/100) & Loss 149.01522827148438\n",
            "Epochs(289/100) & Loss 148.647705078125\n",
            "Epochs(290/100) & Loss 148.28213500976562\n",
            "Epochs(291/100) & Loss 147.9185028076172\n",
            "Epochs(292/100) & Loss 147.55667114257812\n",
            "Epochs(293/100) & Loss 147.19644165039062\n",
            "Epochs(294/100) & Loss 146.8380584716797\n",
            "Epochs(295/100) & Loss 146.4815216064453\n",
            "Epochs(296/100) & Loss 146.12684631347656\n",
            "Epochs(297/100) & Loss 145.77362060546875\n",
            "Epochs(298/100) & Loss 145.42236328125\n",
            "Epochs(299/100) & Loss 145.07257080078125\n",
            "Epochs(300/100) & Loss 144.72451782226562\n",
            "Epochs(301/100) & Loss 144.37832641601562\n",
            "Epochs(302/100) & Loss 144.0337677001953\n",
            "Epochs(303/100) & Loss 143.69052124023438\n",
            "Epochs(304/100) & Loss 143.34912109375\n",
            "Epochs(305/100) & Loss 143.00918579101562\n",
            "Epochs(306/100) & Loss 142.671142578125\n",
            "Epochs(307/100) & Loss 142.3344268798828\n",
            "Epochs(308/100) & Loss 141.99929809570312\n",
            "Epochs(309/100) & Loss 141.66574096679688\n",
            "Epochs(310/100) & Loss 141.33375549316406\n",
            "Epochs(311/100) & Loss 141.00320434570312\n",
            "Epochs(312/100) & Loss 140.67410278320312\n",
            "Epochs(313/100) & Loss 140.34671020507812\n",
            "Epochs(314/100) & Loss 140.02053833007812\n",
            "Epochs(315/100) & Loss 139.69589233398438\n",
            "Epochs(316/100) & Loss 139.37266540527344\n",
            "Epochs(317/100) & Loss 139.05093383789062\n",
            "Epochs(318/100) & Loss 138.73062133789062\n",
            "Epochs(319/100) & Loss 138.41183471679688\n",
            "Epochs(320/100) & Loss 138.09426879882812\n",
            "Epochs(321/100) & Loss 137.7781219482422\n",
            "Epochs(322/100) & Loss 137.46340942382812\n",
            "Epochs(323/100) & Loss 137.1498565673828\n",
            "Epochs(324/100) & Loss 136.8378143310547\n",
            "Epochs(325/100) & Loss 136.52713012695312\n",
            "Epochs(326/100) & Loss 136.21762084960938\n",
            "Epochs(327/100) & Loss 135.90951538085938\n",
            "Epochs(328/100) & Loss 135.60267639160156\n",
            "Epochs(329/100) & Loss 135.29727172851562\n",
            "Epochs(330/100) & Loss 134.99282836914062\n",
            "Epochs(331/100) & Loss 134.6898651123047\n",
            "Epochs(332/100) & Loss 134.38815307617188\n",
            "Epochs(333/100) & Loss 134.087646484375\n",
            "Epochs(334/100) & Loss 133.788330078125\n",
            "Epochs(335/100) & Loss 133.49032592773438\n",
            "Epochs(336/100) & Loss 133.19332885742188\n",
            "Epochs(337/100) & Loss 132.89761352539062\n",
            "Epochs(338/100) & Loss 132.6031951904297\n",
            "Epochs(339/100) & Loss 132.30999755859375\n",
            "Epochs(340/100) & Loss 132.017822265625\n",
            "Epochs(341/100) & Loss 131.7267608642578\n",
            "Epochs(342/100) & Loss 131.43701171875\n",
            "Epochs(343/100) & Loss 131.14828491210938\n",
            "Epochs(344/100) & Loss 130.86068725585938\n",
            "Epochs(345/100) & Loss 130.5743408203125\n",
            "Epochs(346/100) & Loss 130.28890991210938\n",
            "Epochs(347/100) & Loss 130.00469970703125\n",
            "Epochs(348/100) & Loss 129.72145080566406\n",
            "Epochs(349/100) & Loss 129.43948364257812\n",
            "Epochs(350/100) & Loss 129.158447265625\n",
            "Epochs(351/100) & Loss 128.87850952148438\n",
            "Epochs(352/100) & Loss 128.59951782226562\n",
            "Epochs(353/100) & Loss 128.32171630859375\n",
            "Epochs(354/100) & Loss 128.044921875\n",
            "Epochs(355/100) & Loss 127.76904296875\n",
            "Epochs(356/100) & Loss 127.494384765625\n",
            "Epochs(357/100) & Loss 127.2206802368164\n",
            "Epochs(358/100) & Loss 126.94783782958984\n",
            "Epochs(359/100) & Loss 126.6761245727539\n",
            "Epochs(360/100) & Loss 126.4052734375\n",
            "Epochs(361/100) & Loss 126.13542175292969\n",
            "Epochs(362/100) & Loss 125.86669921875\n",
            "Epochs(363/100) & Loss 125.5987319946289\n",
            "Epochs(364/100) & Loss 125.33192443847656\n",
            "Epochs(365/100) & Loss 125.0660400390625\n",
            "Epochs(366/100) & Loss 124.8008804321289\n",
            "Epochs(367/100) & Loss 124.5368881225586\n",
            "Epochs(368/100) & Loss 124.27375793457031\n",
            "Epochs(369/100) & Loss 124.01142883300781\n",
            "Epochs(370/100) & Loss 123.7502212524414\n",
            "Epochs(371/100) & Loss 123.48973083496094\n",
            "Epochs(372/100) & Loss 123.23014831542969\n",
            "Epochs(373/100) & Loss 122.9717025756836\n",
            "Epochs(374/100) & Loss 122.71388244628906\n",
            "Epochs(375/100) & Loss 122.45704650878906\n",
            "Epochs(376/100) & Loss 122.20097351074219\n",
            "Epochs(377/100) & Loss 121.94586181640625\n",
            "Epochs(378/100) & Loss 121.6915512084961\n",
            "Epochs(379/100) & Loss 121.4381332397461\n",
            "Epochs(380/100) & Loss 121.18553161621094\n",
            "Epochs(381/100) & Loss 120.93379974365234\n",
            "Epochs(382/100) & Loss 120.68284606933594\n",
            "Epochs(383/100) & Loss 120.432861328125\n",
            "Epochs(384/100) & Loss 120.18357849121094\n",
            "Epochs(385/100) & Loss 119.9351806640625\n",
            "Epochs(386/100) & Loss 119.6874008178711\n",
            "Epochs(387/100) & Loss 119.4406967163086\n",
            "Epochs(388/100) & Loss 119.19465637207031\n",
            "Epochs(389/100) & Loss 118.94938659667969\n",
            "Epochs(390/100) & Loss 118.70491027832031\n",
            "Epochs(391/100) & Loss 118.46119689941406\n",
            "Epochs(392/100) & Loss 118.21833801269531\n",
            "Epochs(393/100) & Loss 117.97618103027344\n",
            "Epochs(394/100) & Loss 117.73478698730469\n",
            "Epochs(395/100) & Loss 117.4941635131836\n",
            "Epochs(396/100) & Loss 117.2542953491211\n",
            "Epochs(397/100) & Loss 117.01531982421875\n",
            "Epochs(398/100) & Loss 116.7768325805664\n",
            "Epochs(399/100) & Loss 116.53916931152344\n"
          ]
        }
      ],
      "source": [
        "# Training for muliple epochs\n",
        "for i in range(400):\n",
        "  preds = model(inputs)\n",
        "  loss = MSE(target, preds)\n",
        "  loss.backward()\n",
        "  with torch.no_grad():\n",
        "    w -= w.grad * 1e-5\n",
        "    b -= b.grad * 1e-5\n",
        "\n",
        "    w.grad.zero_()\n",
        "    b.grad.zero_()\n",
        "  print(f\"Epochs({i}/{100}) & Loss {loss}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 79,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DO_bRU6Qz86y",
        "outputId": "351198b8-c77a-4415-fccc-1d98c482a22d"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "tensor(116.3022, grad_fn=<DivBackward0>)\n"
          ]
        }
      ],
      "source": [
        "preds = model(inputs)\n",
        "loss = MSE(target, preds)\n",
        "print(loss)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lXBDZCOD04ld"
      },
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": 80,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Y-lDFge00KLx",
        "outputId": "790d9b8f-c229-48dc-c1bc-f28d16cdc4d3"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "10.784350140216022"
            ]
          },
          "execution_count": 80,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "from math import sqrt\n",
        "sqrt(loss)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 81,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SPUtWQwQ0OdG",
        "outputId": "37336057-2466-46db-c86c-ba09aa05c580"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "tensor([[ 58.7382,  67.6438],\n",
              "        [ 72.7201,  97.5322],\n",
              "        [137.7358, 119.6289],\n",
              "        [ 30.4782,  41.4395],\n",
              "        [ 79.6659, 112.8073]], grad_fn=<AddBackward0>)"
            ]
          },
          "execution_count": 81,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "preds"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 82,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nL8eYlYb0P5i",
        "outputId": "b2009b7e-c771-49ec-ed29-76677f6cb7bb"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "tensor([[ 56.,  70.],\n",
              "        [ 81., 101.],\n",
              "        [119., 113.],\n",
              "        [ 22.,  37.],\n",
              "        [103., 119.]])"
            ]
          },
          "execution_count": 82,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "target"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "sKJPiCB00SEw"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ad8zpHxw05V-"
      },
      "source": [
        "# Fashion MNIST Neural Net example using Pytorch"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "1SOick-F09s6"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "from torch import nn\n",
        "from torch.utils.data import DataLoader\n",
        "from torchvision import datasets\n",
        "from torchvision.transforms import ToTensor, Lambda, Compose\n",
        "import matplotlib.pyplot as plt\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_SXyjWQa1RcD",
        "outputId": "40b550c2-e245-47a6-e396-cb1d88975943"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Downloading http://fashion-mnist.s3-website.eu-central-1.amazonaws.com/train-images-idx3-ubyte.gz\n",
            "Downloading http://fashion-mnist.s3-website.eu-central-1.amazonaws.com/train-images-idx3-ubyte.gz to data/FashionMNIST/raw/train-images-idx3-ubyte.gz\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|| 26.4M/26.4M [00:00<00:00, 113MB/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Extracting data/FashionMNIST/raw/train-images-idx3-ubyte.gz to data/FashionMNIST/raw\n",
            "\n",
            "Downloading http://fashion-mnist.s3-website.eu-central-1.amazonaws.com/train-labels-idx1-ubyte.gz\n",
            "Downloading http://fashion-mnist.s3-website.eu-central-1.amazonaws.com/train-labels-idx1-ubyte.gz to data/FashionMNIST/raw/train-labels-idx1-ubyte.gz\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|| 29.5k/29.5k [00:00<00:00, 5.05MB/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Extracting data/FashionMNIST/raw/train-labels-idx1-ubyte.gz to data/FashionMNIST/raw\n",
            "\n",
            "Downloading http://fashion-mnist.s3-website.eu-central-1.amazonaws.com/t10k-images-idx3-ubyte.gz\n",
            "Downloading http://fashion-mnist.s3-website.eu-central-1.amazonaws.com/t10k-images-idx3-ubyte.gz to data/FashionMNIST/raw/t10k-images-idx3-ubyte.gz\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|| 4.42M/4.42M [00:00<00:00, 55.9MB/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Extracting data/FashionMNIST/raw/t10k-images-idx3-ubyte.gz to data/FashionMNIST/raw\n",
            "\n",
            "Downloading http://fashion-mnist.s3-website.eu-central-1.amazonaws.com/t10k-labels-idx1-ubyte.gz\n",
            "Downloading http://fashion-mnist.s3-website.eu-central-1.amazonaws.com/t10k-labels-idx1-ubyte.gz to data/FashionMNIST/raw/t10k-labels-idx1-ubyte.gz\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|| 5.15k/5.15k [00:00<00:00, 6.31MB/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Extracting data/FashionMNIST/raw/t10k-labels-idx1-ubyte.gz to data/FashionMNIST/raw\n",
            "\n"
          ]
        }
      ],
      "source": [
        "# Download training data from open datasets\n",
        "\n",
        "training_data = datasets.FashionMNIST(\n",
        "    root = 'data',\n",
        "    train= True,\n",
        "    download = True,\n",
        "    transform = ToTensor(),\n",
        ")\n",
        "\n",
        "# Download test data from open datasets\n",
        "\n",
        "test_data = datasets.FashionMNIST(\n",
        "    root=\"data\",\n",
        "    train=False,\n",
        "    download = True,\n",
        "    transform = ToTensor(),\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 203
        },
        "id": "9wimdMsj1t_n",
        "outputId": "34a44c54-c6d8-4642-bede-c430c7d958e3"
      },
      "outputs": [
        {
          "data": {
            "text/html": [
              "<div style=\"max-width:800px; border: 1px solid var(--colab-border-color);\"><style>\n",
              "      pre.function-repr-contents {\n",
              "        overflow-x: auto;\n",
              "        padding: 8px 12px;\n",
              "        max-height: 500px;\n",
              "      }\n",
              "\n",
              "      pre.function-repr-contents.function-repr-contents-collapsed {\n",
              "        cursor: pointer;\n",
              "        max-height: 100px;\n",
              "      }\n",
              "    </style>\n",
              "    <pre style=\"white-space: initial; background:\n",
              "         var(--colab-secondary-surface-color); padding: 8px 12px;\n",
              "         border-bottom: 1px solid var(--colab-border-color);\"><b>torchvision.datasets.mnist.FashionMNIST</b><br/>def __init__(root: Union[str, Path], train: bool=True, transform: Optional[Callable]=None, target_transform: Optional[Callable]=None, download: bool=False) -&gt; None</pre><pre class=\"function-repr-contents function-repr-contents-collapsed\" style=\"\"><a class=\"filepath\" style=\"display:none\" href=\"#\">/usr/local/lib/python3.11/dist-packages/torchvision/datasets/mnist.py</a>`Fashion-MNIST &lt;https://github.com/zalandoresearch/fashion-mnist&gt;`_ Dataset.\n",
              "\n",
              "Args:\n",
              "    root (str or ``pathlib.Path``): Root directory of dataset where ``FashionMNIST/raw/train-images-idx3-ubyte``\n",
              "        and  ``FashionMNIST/raw/t10k-images-idx3-ubyte`` exist.\n",
              "    train (bool, optional): If True, creates dataset from ``train-images-idx3-ubyte``,\n",
              "        otherwise from ``t10k-images-idx3-ubyte``.\n",
              "    download (bool, optional): If True, downloads the dataset from the internet and\n",
              "        puts it in root directory. If dataset is already downloaded, it is not\n",
              "        downloaded again.\n",
              "    transform (callable, optional): A function/transform that  takes in a PIL image\n",
              "        and returns a transformed version. E.g, ``transforms.RandomCrop``\n",
              "    target_transform (callable, optional): A function/transform that takes in the\n",
              "        target and transforms it.</pre>\n",
              "      <script>\n",
              "      if (google.colab.kernel.accessAllowed && google.colab.files && google.colab.files.view) {\n",
              "        for (const element of document.querySelectorAll('.filepath')) {\n",
              "          element.style.display = 'block'\n",
              "          element.onclick = (event) => {\n",
              "            event.preventDefault();\n",
              "            event.stopPropagation();\n",
              "            google.colab.files.view(element.textContent, 203);\n",
              "          };\n",
              "        }\n",
              "      }\n",
              "      for (const element of document.querySelectorAll('.function-repr-contents')) {\n",
              "        element.onclick = (event) => {\n",
              "          event.preventDefault();\n",
              "          event.stopPropagation();\n",
              "          element.classList.toggle('function-repr-contents-collapsed');\n",
              "        };\n",
              "      }\n",
              "      </script>\n",
              "      </div>"
            ],
            "text/plain": [
              "torchvision.datasets.mnist.FashionMNIST"
            ]
          },
          "execution_count": 3,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "type(training_data)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "J3Rnb1uN15BX",
        "outputId": "0b5fc727-d139-477c-b8ac-244bcb7e44a4"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Shape of X [N, C, H, W]  torch.Size([64, 1, 28, 28])\n",
            "Shape of y:  torch.Size([64]) torch.int64\n"
          ]
        }
      ],
      "source": [
        "batch_size = 64\n",
        "\n",
        "# Create data loaders\n",
        "\n",
        "train_dataloader = DataLoader(training_data, batch_size = batch_size)\n",
        "test_dataloader = DataLoader(test_data, batch_size = batch_size)\n",
        "\n",
        "for X, y in test_dataloader:\n",
        "  print(\"Shape of X [N, C, H, W] \", X.shape)\n",
        "  print(\"Shape of y: \", y.shape, y.dtype)\n",
        "  break"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0X3IHdWa2fcP",
        "outputId": "1fd01ab5-2d3a-4b96-e2fe-0cab59fa0a13"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Using cuda device\n"
          ]
        }
      ],
      "source": [
        "# Get cpu or gpu device for training\n",
        "\n",
        "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "print(f\"Using {device} device\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZFAJGezT3Kdb",
        "outputId": "9d3105ef-3c1a-4e4a-944e-56a87bea9fe6"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "NeuralNetwork(\n",
            "  (flatten): Flatten(start_dim=1, end_dim=-1)\n",
            "  (linear_relu_stack): Sequential(\n",
            "    (0): Linear(in_features=784, out_features=512, bias=True)\n",
            "    (1): ReLU()\n",
            "    (2): Linear(in_features=512, out_features=512, bias=True)\n",
            "    (3): ReLU()\n",
            "    (4): Linear(in_features=512, out_features=10, bias=True)\n",
            "  )\n",
            ")\n"
          ]
        }
      ],
      "source": [
        "# Define the NN Model\n",
        "class NeuralNetwork(nn.Module):\n",
        "\n",
        "  def __init__(self):\n",
        "    super(NeuralNetwork, self).__init__()\n",
        "\n",
        "    self.flatten = nn.Flatten()\n",
        "\n",
        "    # Hidden Layers with ReLU activation function\n",
        "    self.linear_relu_stack = nn.Sequential(\n",
        "        nn.Linear(28*28, 512),\n",
        "        nn.ReLU(),\n",
        "        nn.Linear(512, 512),\n",
        "        nn.ReLU(),\n",
        "        nn.Linear(512, 10) # Output layer\n",
        "    )\n",
        "\n",
        "  def forward(self, x):\n",
        "    x = self.flatten(x)\n",
        "    logits = self.linear_relu_stack(x)\n",
        "    return logits\n",
        "\n",
        "\n",
        "model= NeuralNetwork().to(device)\n",
        "\n",
        "print(model)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "Fq-CRg9C48WQ"
      },
      "outputs": [],
      "source": [
        "# Cross Entropy Loss ----> Because it is a multiclass classification problem\n",
        "\n",
        "loss_fn = nn.CrossEntropyLoss()\n",
        "\n",
        "# Optimizer ---> SGD ---> Stochastic Gradient Descent\n",
        "# lr = Learning Rate\n",
        "optimizer = torch.optim.SGD(model.parameters(), lr = 1e-3)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "x4-NOviK5dCW"
      },
      "outputs": [],
      "source": [
        "# Model Training\n",
        "\n",
        "def train(dataloader, model, loss_fn, optimizer):\n",
        "  size = len(dataloader.dataset)\n",
        "  model.train()\n",
        "\n",
        "  for batch, (X, y) in enumerate(dataloader):\n",
        "    X, y= X.to(device), y.to(device) # related to gpu computation\n",
        "\n",
        "    # Compute prediction error\n",
        "    pred = model(X)\n",
        "    loss = loss_fn(pred, y)\n",
        "\n",
        "    # BackPropagation\n",
        "    optimizer.zero_grad()\n",
        "    loss.backward()\n",
        "    optimizer.step()\n",
        "\n",
        "    if batch %100 ==0 :\n",
        "      loss, current = loss.item(), batch * len(X)\n",
        "      print(f\"Loss: {loss} [{current}/{size}]\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "id": "Ew2nGMBN6hvZ"
      },
      "outputs": [],
      "source": [
        "def test(dataloader, model, loss_fn):\n",
        "  size = len(dataloader.dataset)\n",
        "\n",
        "  num_batches = len(dataloader)\n",
        "\n",
        "  model.eval()\n",
        "\n",
        "  test_loss, correct = 0, 0\n",
        "\n",
        "  with torch.no_grad():\n",
        "    for X, y in dataloader:\n",
        "      X, y = X.to(device), y.to(device)\n",
        "      pred = model(X)\n",
        "\n",
        "      test_loss += loss_fn(pred, y).item()\n",
        "      correct += (pred.argmax(1) == y).type(torch.float).sum().item()\n",
        "\n",
        "  test_loss /= num_batches # average loss per batch\n",
        "  correct /= size # %age of correct predictions or accuracy\n",
        "\n",
        "  print(f\"Test Error: \\n Accuracy: {100*correct} %, Avg loss {test_loss}\\n\")\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0NyGltnt7wam",
        "outputId": "3c051188-087a-43a7-ab26-297cc93e3b73"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 1 \n",
            " --------------------------\n",
            "Loss: 0.7453593611717224 [0/60000]\n",
            "Loss: 0.8441538214683533 [6400/60000]\n",
            "Loss: 0.6057318449020386 [12800/60000]\n",
            "Loss: 0.8170516490936279 [19200/60000]\n",
            "Loss: 0.730737030506134 [25600/60000]\n",
            "Loss: 0.6974841952323914 [32000/60000]\n",
            "Loss: 0.8081368207931519 [38400/60000]\n",
            "Loss: 0.7774155139923096 [44800/60000]\n",
            "Loss: 0.7685599327087402 [51200/60000]\n",
            "Loss: 0.7447283267974854 [57600/60000]\n",
            "Test Error: \n",
            " Accuracy: 73.24000000000001 %, Avg loss 0.7387217583170362\n",
            "\n",
            "Done\n",
            "\n"
          ]
        }
      ],
      "source": [
        "epochs = 1\n",
        "for t in range(epochs):\n",
        "  print(f\"Epoch {t+1} \\n --------------------------\")\n",
        "  train(train_dataloader, model, loss_fn, optimizer)\n",
        "  test(test_dataloader, model, loss_fn)\n",
        "\n",
        "print(\"Done\\n\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "sczjxn9f8_DE"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "w8DHTQAO8Ifr",
        "outputId": "61b7b176-dd84-4bc5-fc9c-5a266561af1b"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Saved model state to model.pth\n"
          ]
        }
      ],
      "source": [
        "#save model\n",
        "torch.save(model.state_dict(), \"model.pth\")\n",
        "print(\"Saved model state to model.pth\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 25,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "i1oNvFeR9B5g",
        "outputId": "7644baa9-47ce-405d-8e57-0dc6d1c72930"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Predicted: Coat Actual: Coat\n"
          ]
        }
      ],
      "source": [
        "## Prediction\n",
        "\n",
        "classes = [\n",
        "    \"T-shirt/top\",\n",
        "\n",
        "\"Trouser\",\n",
        "\n",
        "\"Pullover\",\n",
        "\n",
        "\"Dress\",\n",
        "\n",
        "\"Coat\",\n",
        "\n",
        "\"Sandal\",\n",
        "\n",
        "\"Shirt\",\n",
        "\n",
        "\"Sneaker\",\n",
        "\n",
        "\"Bag\",\n",
        "\n",
        "\"Ankle boot\"\n",
        "\n",
        "]\n",
        "\n",
        "\n",
        "model.eval()\n",
        "\n",
        "x, y = test_data[10][0], test_data[10][1]\n",
        "x = x.to(device)\n",
        "# y = y.to(device)\n",
        "with torch.no_grad():\n",
        "  pred = model(x)\n",
        "  predicted, actual = classes[pred[0].argmax(0)], classes[y]\n",
        "\n",
        "  print(f\"Predicted: {predicted} Actual: {actual}\")\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 26,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 430
        },
        "id": "b0IdzJmy95yG",
        "outputId": "7a853b2c-641a-4649-f0cd-c0894c45bddb"
      },
      "outputs": [
        {
          "data": {
            "image/png": "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",
            "text/plain": [
              "<Figure size 640x480 with 1 Axes>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Assuming 'x' contains the image data\n",
        "plt.imshow(x.cpu().squeeze())  # Move tensor to CPU if using GPU\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "p7Les0pa-QFX"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
