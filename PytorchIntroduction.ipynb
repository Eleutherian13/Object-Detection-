{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "70bd9b6f",
   "metadata": {},
   "source": [
    "# PyTorch Basics: Tensors & Gradients\n",
    "\n",
    "<img src=\"data:image/png;base64,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\" width=\"600\"\n",
    "     height=\"300\">\n",
    "\n",
    "\n",
    "\n",
    "  PyTorch is an open-source machine learning framework that is primarily used for developing and training deep learning models. It was developed by Facebook's AI Research Lab and released in 2016. PyTorch provides a flexible and dynamic approach to building neural networks, making it a popular choice among researchers and developers.\n",
    "\n",
    "The framework is built on a dynamic computational graph concept, which means that the graph is built and modified on-the-fly as the program runs. This allows for more intuitive and flexible model development, as you can use standard Python control flow statements and debug the model easily.\n",
    "\n",
    "PyTorch supports automatic differentiation, which enables efficient computation of gradients for training neural networks using backpropagation. It provides a rich set of tools and libraries for tasks such as data loading, model building, optimization, and evaluation.\n",
    "\n",
    "One of the key advantages of PyTorch is its support for GPU acceleration, allowing you to train models on GPUs to significantly speed up computations. It also has a large and active community, which means there are plenty of resources, tutorials, and pre-trained models available.\n",
    "\n",
    "PyTorch is often compared to TensorFlow, another popular deep learning framework. While TensorFlow focuses more on static computation graphs, PyTorch emphasizes dynamic computation graphs. This fundamental difference in design philosophy gives PyTorch an edge when it comes to flexibility and ease of use.\n",
    "\n",
    "Overall, PyTorch is widely used in the research community and is gaining popularity in industry applications as well. It provides a powerful and user-friendly platform for building and training deep learning models.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d8612c00",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Looking in indexes: https://pypi.org/simple/\n",
      "Requirement already satisfied: torch in c:\\users\\manas\\appdata\\roaming\\python\\python314\\site-packages (2.9.1)\n",
      "Requirement already satisfied: filelock in c:\\users\\manas\\appdata\\roaming\\python\\python314\\site-packages (from torch) (3.20.2)\n",
      "Requirement already satisfied: typing-extensions>=4.10.0 in c:\\users\\manas\\appdata\\roaming\\python\\python314\\site-packages (from torch) (4.15.0)\n",
      "Requirement already satisfied: sympy>=1.13.3 in c:\\users\\manas\\appdata\\roaming\\python\\python314\\site-packages (from torch) (1.14.0)\n",
      "Requirement already satisfied: networkx>=2.5.1 in c:\\users\\manas\\appdata\\roaming\\python\\python314\\site-packages (from torch) (3.6.1)\n",
      "Requirement already satisfied: jinja2 in c:\\users\\manas\\appdata\\roaming\\python\\python314\\site-packages (from torch) (3.1.6)\n",
      "Requirement already satisfied: fsspec>=0.8.5 in c:\\users\\manas\\appdata\\roaming\\python\\python314\\site-packages (from torch) (2025.12.0)\n",
      "Requirement already satisfied: setuptools in c:\\users\\manas\\appdata\\roaming\\python\\python314\\site-packages (from torch) (80.9.0)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in c:\\users\\manas\\appdata\\roaming\\python\\python314\\site-packages (from sympy>=1.13.3->torch) (1.3.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in c:\\users\\manas\\appdata\\roaming\\python\\python314\\site-packages (from jinja2->torch) (3.0.3)\n"
     ]
    }
   ],
   "source": [
    "!pip install torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e2d8a757",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "2500c6ab",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([1, 2, 4]) torch.Size([3])\n",
      "\n",
      "\n",
      "torch.int64\n"
     ]
    }
   ],
   "source": [
    "# ------------- \n",
    "# pytorch basics \n",
    "# --------------\n",
    "tens1 = torch.tensor([1,2,4])       # this is also called as 2d vecotr\n",
    "tens1\n",
    "print(tens1 , tens1.shape , end='\\n\\n\\n')\n",
    "print(tens1.dtype)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "9e51ba68",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 5.,  6.],\n",
       "        [ 7.,  8.],\n",
       "        [ 9., 10.]])"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Matrix\n",
    "t3 = torch.tensor([\n",
    "    [5., 6],\n",
    "    [7,8],\n",
    "    [9,10]\n",
    "])\n",
    "t3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "57bcae55",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[11, 12, 13],\n",
       "         [14, 15, 16]],\n",
       "\n",
       "        [[17, 18, 19],\n",
       "         [20, 21, 22]]])"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 3 dimensional array\n",
    "t4 = torch.tensor([\n",
    "    [\n",
    "        [11, 12, 13],\n",
    "        [14, 15, 16]\n",
    "    ],\n",
    "    [\n",
    "        [17,18,19],\n",
    "        [20,21,22]\n",
    "    ]\n",
    "])\n",
    "\n",
    "t4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "40c2e140",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([2, 2, 3])\n"
     ]
    }
   ],
   "source": [
    "print(t4.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "2ad2a070",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor(4., requires_grad=True),\n",
       " tensor(4., requires_grad=True),\n",
       " tensor(5., requires_grad=True))"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# create tensors \n",
    "\n",
    "X = torch.tensor(4. , requires_grad=True)\n",
    "W = torch.tensor(4. , requires_grad=True)\n",
    "B = torch.tensor(5., requires_grad=True)\n",
    "X , W , B"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "25e2597f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(4.)\n",
      "tensor(4.)\n",
      "tensor(1.)\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "X = torch.tensor(4., requires_grad=True)\n",
    "W = torch.tensor(4., requires_grad=True)\n",
    "B = torch.tensor(5., requires_grad=True)\n",
    "\n",
    "y = W * X + B   # simple linear function\n",
    "\n",
    "y.backward()    # compute gradients\n",
    "\n",
    "\n",
    "print(X.grad)  # dy/dX = W = 4\n",
    "print(W.grad)  # dy/dW = X = 4\n",
    "print(B.grad)  # dy/dB = 1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "fe6b7ef8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(4.)\n"
     ]
    }
   ],
   "source": [
    "print(X.grad) # prints the dy / dx\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "417203d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "X = torch.tensor(4., requires_grad=False)\n",
    "W = torch.tensor(4., requires_grad=False)\n",
    "\n",
    "y = X * W\n",
    "y.backward()  # ❌ This will raise an error\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "989f2114",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "False\n"
     ]
    }
   ],
   "source": [
    "X = torch.tensor(4., requires_grad=True)\n",
    "\n",
    "with torch.no_grad():\n",
    "    y = X * 3\n",
    "\n",
    "\n",
    "print(y.requires_grad)  # False\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "5af7da16",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n",
      "{}\n",
      "<built-in method count of tuple object at 0x000002901DEE5FC0>\n",
      "<built-in method register_hook of AddBackward0 object at 0x00000290658189D0>\n",
      "<built-in method register_prehook of AddBackward0 object at 0x000002906581B4F0>\n"
     ]
    }
   ],
   "source": [
    "X = torch.tensor(2., requires_grad=True)\n",
    "y = X**2 + 3*X\n",
    "# You’ll see something like <AddBackward0> showing PyTorch built a computational graph for the operations you've performed on the tensors \n",
    "print(y.grad_fn.requires_grad)\n",
    "print(y.grad_fn.metadata)\n",
    "print(y.grad_fn.next_functions.count)  \n",
    "print(y.grad_fn.register_hook)  \n",
    "print(y.grad_fn.register_prehook)  \n",
    "# print(y.grad_fn.remove_hook)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "ce77d62a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(4., requires_grad=True)"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X = torch.tensor(4.)\n",
    "X.requires_grad_()  # now it tracks gradients\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c854c084",
   "metadata": {},
   "source": [
    "Good follow-up — this is exactly the right thing to be confused about.\n",
    "\n",
    "### What this code does\n",
    "\n",
    "```python\n",
    "X = torch.tensor(4., requires_grad=True)\n",
    "\n",
    "with torch.no_grad():\n",
    "    y = X * 3\n",
    "\n",
    "print(y.requires_grad)  # False\n",
    "```\n",
    "\n",
    "Even though `X` has `requires_grad=True`, the line:\n",
    "\n",
    "```python\n",
    "with torch.no_grad():\n",
    "```\n",
    "\n",
    "**temporarily tells PyTorch to stop tracking gradients** for everything inside that block.\n",
    "\n",
    "So:\n",
    "\n",
    "* Normally, `y = X * 3` would be tracked and\n",
    "  → `y.requires_grad == True`\n",
    "* But inside `torch.no_grad()`\n",
    "  → PyTorch does **not build a computation graph**\n",
    "  → `y.requires_grad == False`\n",
    "\n",
    "---\n",
    "\n",
    "### Why this is useful\n",
    "\n",
    "#### 1. Faster code\n",
    "\n",
    "No gradient tracking = less memory + faster execution.\n",
    "\n",
    "#### 2. Inference mode\n",
    "\n",
    "When you're using a trained model to make predictions:\n",
    "\n",
    "```python\n",
    "with torch.no_grad():\n",
    "    predictions = model(inputs)\n",
    "```\n",
    "\n",
    "You don’t need gradients because you're not training.\n",
    "\n",
    "---\n",
    "\n",
    "### Think of it like this\n",
    "\n",
    "`requires_grad=True` = \"Record everything so I can take derivatives later\"\n",
    "`torch.no_grad()` = \"For now, stop recording anything\"\n",
    "\n",
    "Even if a tensor *can* track gradients, `torch.no_grad()` overrides that behavior temporarily.\n",
    "\n",
    "---\n",
    "\n",
    "### Quick mental example\n",
    "\n",
    "Without `no_grad`:\n",
    "\n",
    "```python\n",
    "X = torch.tensor(4., requires_grad=True)\n",
    "y = X * 3\n",
    "print(y.requires_grad)  # True\n",
    "```\n",
    "\n",
    "With `no_grad`:\n",
    "\n",
    "```python\n",
    "with torch.no_grad():\n",
    "    y = X * 3\n",
    "print(y.requires_grad)  # False\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "If you want, I can also explain:\n",
    "\n",
    "* Difference between `torch.no_grad()` and `.detach()`\n",
    "* Why gradients sometimes become `None`\n",
    "* How this affects neural network training loops\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4c3b542c",
   "metadata": {},
   "source": [
    "Yes — PyTorch provides a few **similar mechanisms to control gradient tracking**, and each has a slightly different use case. Here are the most important ones you’ll see in practice.\n",
    "\n",
    "---\n",
    "\n",
    "## 1. `torch.no_grad()` (you already saw this)\n",
    "\n",
    "**Purpose:** Temporarily disable gradient tracking.\n",
    "\n",
    "```python\n",
    "with torch.no_grad():\n",
    "    y = model(x)\n",
    "```\n",
    "\n",
    "* No computation graph is built\n",
    "* Faster and uses less memory\n",
    "* Commonly used during **evaluation / inference**\n",
    "\n",
    "---\n",
    "\n",
    "## 2. `torch.set_grad_enabled(True/False)`\n",
    "\n",
    "This is like a more flexible version of `no_grad`.\n",
    "\n",
    "```python\n",
    "torch.set_grad_enabled(False)  # turn OFF grads\n",
    "y = X * 3\n",
    "\n",
    "torch.set_grad_enabled(True)   # turn ON grads again\n",
    "```\n",
    "\n",
    "Often used like this:\n",
    "\n",
    "```python\n",
    "is_train = False\n",
    "\n",
    "with torch.set_grad_enabled(is_train):\n",
    "    y = model(x)\n",
    "```\n",
    "\n",
    "Useful when writing code that works for both training and evaluation.\n",
    "\n",
    "---\n",
    "\n",
    "## 3. `.detach()`\n",
    "\n",
    "Instead of disabling grads for a block, this works on a **specific tensor**.\n",
    "\n",
    "```python\n",
    "X = torch.tensor(4., requires_grad=True)\n",
    "\n",
    "y = X * 3\n",
    "z = y.detach()\n",
    "\n",
    "print(z.requires_grad)  # False\n",
    "```\n",
    "\n",
    "`detach()` means:\n",
    "\n",
    "> \"Give me a new tensor with the same value, but cut it off from the graph.\"\n",
    "\n",
    "Very common in advanced models (GANs, RL, etc.).\n",
    "\n",
    "---\n",
    "\n",
    "## 4. `.requires_grad_(False)`\n",
    "\n",
    "This permanently changes a tensor’s behavior.\n",
    "\n",
    "```python\n",
    "X = torch.tensor(4., requires_grad=True)\n",
    "X.requires_grad_(False)\n",
    "\n",
    "print(X.requires_grad)  # False\n",
    "```\n",
    "\n",
    "Useful when you want to **freeze model parameters**, e.g.:\n",
    "\n",
    "```python\n",
    "for param in model.parameters():\n",
    "    param.requires_grad = False\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "## 5. `torch.inference_mode()` (even stronger than `no_grad`)\n",
    "\n",
    "```python\n",
    "with torch.inference_mode():\n",
    "    y = model(x)\n",
    "```\n",
    "\n",
    "* Like `no_grad()`, but **even faster**\n",
    "* Prevents accidental graph building\n",
    "* Best for production inference\n",
    "\n",
    "---\n",
    "\n",
    "## Quick comparison\n",
    "\n",
    "| Method                     | Scope           | Typical use             |\n",
    "| -------------------------- | --------------- | ----------------------- |\n",
    "| `torch.no_grad()`          | Block           | Validation / inference  |\n",
    "| `torch.set_grad_enabled()` | Block (dynamic) | Training/eval switch    |\n",
    "| `.detach()`                | Single tensor   | Cut graph for one value |\n",
    "| `.requires_grad_(False)`   | Tensor          | Freeze weights          |\n",
    "| `torch.inference_mode()`   | Block           | Fast, safe inference    |\n",
    "\n",
    "---\n",
    "\n",
    "If you want, I can next explain this visually with a **computation graph diagram** — that usually makes everything click.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d86c7a6",
   "metadata": {
    "vscode": {
     "languageId": "markdown"
    }
   },
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "invalid character '—' (U+2014) (3730257853.py, line 1)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;36m  Cell \u001b[1;32mIn[35], line 1\u001b[1;36m\u001b[0m\n\u001b[1;33m    Yes — PyTorch provides a few **similar mechanisms to control gradient tracking**, and each has a slightly different use case. Here are the most important ones you’ll see in practice.\u001b[0m\n\u001b[1;37m        ^\u001b[0m\n\u001b[1;31mSyntaxError\u001b[0m\u001b[1;31m:\u001b[0m invalid character '—' (U+2014)\n"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d39b8b6b",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
